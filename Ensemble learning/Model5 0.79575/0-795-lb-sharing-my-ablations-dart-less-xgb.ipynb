{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c3951949",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2022-07-26T20:03:34.174452Z",
     "iopub.status.busy": "2022-07-26T20:03:34.173610Z",
     "iopub.status.idle": "2022-07-26T20:04:12.840069Z",
     "shell.execute_reply": "2022-07-26T20:04:12.838737Z"
    },
    "papermill": {
     "duration": 38.676327,
     "end_time": "2022-07-26T20:04:12.843204",
     "exception": false,
     "start_time": "2022-07-26T20:03:34.166877",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 (458913, 976)\n",
      "CPU times: user 11.6 s, sys: 4.4 s, total: 16 s\n",
      "Wall time: 38.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%reset -f\n",
    "import gc; gc.collect()\n",
    "\n",
    "import cudf\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "\n",
    "def flatten_columns(df):\n",
    "    df.columns = [\"_\".join(column) for column in df.columns]\n",
    "    return df\n",
    "\n",
    "\n",
    "def preprocess(dataset):\n",
    "    dataset['customer_ID'] = dataset['customer_ID'].str[-16:].str.hex_to_int().astype('int64')\n",
    "    dataset['S_2'] = cudf.to_datetime(dataset['S_2'])\n",
    "    dataset.set_index(['customer_ID', 'S_2'], inplace=True)\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "\n",
    "def engineer(dataset, feature_set):\n",
    "    if feature_set == 0:\n",
    "        dataset = dataset.groupby(level='customer_ID').last()\n",
    "        return dataset\n",
    "    \n",
    "    if feature_set == 1:\n",
    "        cat_cols = [\n",
    "            \"B_30\",\n",
    "            \"B_38\",\n",
    "            \"D_114\",\n",
    "            \"D_116\",\n",
    "            \"D_117\",\n",
    "            \"D_120\",\n",
    "            \"D_126\",\n",
    "            \"D_63\",\n",
    "            \"D_64\",\n",
    "            \"D_66\",\n",
    "            \"D_68\",\n",
    "        ]\n",
    "        cat_feat = dataset[cat_cols].groupby(level='customer_ID').agg(['last']).pipe(flatten_columns)\n",
    "\n",
    "        num_cols = [col for col in dataset.columns if col not in cat_cols + ['target']]\n",
    "        num_feat = dataset[num_cols].groupby(level='customer_ID').agg(['first', 'last', 'mean', 'std']).pipe(flatten_columns)\n",
    "\n",
    "        dataset = cudf.concat([cat_feat, num_feat], axis=1)\n",
    "        return dataset\n",
    "    \n",
    "    if feature_set == 2:\n",
    "        cat_cols = [\n",
    "            \"B_30\",\n",
    "            \"B_38\",\n",
    "            \"D_114\",\n",
    "            \"D_116\",\n",
    "            \"D_117\",\n",
    "            \"D_120\",\n",
    "            \"D_126\",\n",
    "            \"D_63\",\n",
    "            \"D_64\",\n",
    "            \"D_66\",\n",
    "            \"D_68\",\n",
    "        ]\n",
    "        cat_feat = dataset[cat_cols].groupby(level='customer_ID').agg(['last']).pipe(flatten_columns)\n",
    "\n",
    "        num_cols = [col for col in dataset.columns if col not in cat_cols + ['target']]\n",
    "        num_feat = dataset[num_cols].groupby(level='customer_ID').agg(['first', 'last', 'mean', 'std']).pipe(flatten_columns)\n",
    "        \n",
    "        diff_cols_a = [f\"B_{i}\" for i in [11, 14, 17]] + [\"D_39\", \"D_131\"] + [f\"S_{i}\" for i in [16, 23]]\n",
    "        diff_cols_b = [\"P_2\", \"P_3\"]\n",
    "        diff_feat = dataset[diff_cols_a + diff_cols_b]\n",
    "        for a in diff_cols_a:\n",
    "            for b in diff_cols_b:\n",
    "                    diff_feat[f\"{a}-{b}\"] = diff_feat[a] - diff_feat[b]\n",
    "        diff_feat.drop(diff_cols_a + diff_cols_b, axis=1, inplace=True)\n",
    "        diff_feat = diff_feat.groupby(level='customer_ID').agg(['first', 'last', 'mean', 'std']).pipe(flatten_columns)\n",
    "\n",
    "        dataset = cudf.concat([cat_feat, num_feat, diff_feat], axis=1)\n",
    "        return dataset\n",
    "    \n",
    "    if feature_set == 3:\n",
    "        cat_cols = [\n",
    "            \"B_30\",\n",
    "            \"B_38\",\n",
    "            \"D_114\",\n",
    "            \"D_116\",\n",
    "            \"D_117\",\n",
    "            \"D_120\",\n",
    "            \"D_126\",\n",
    "            \"D_63\",\n",
    "            \"D_64\",\n",
    "            \"D_66\",\n",
    "            \"D_68\",\n",
    "        ]\n",
    "        cat_feat = dataset[cat_cols].groupby(level='customer_ID').agg(['count', 'last', 'nunique']).pipe(flatten_columns)\n",
    "\n",
    "        num_cols = [col for col in dataset.columns if col not in cat_cols + ['target']]\n",
    "        num_feat = dataset[num_cols].groupby(level='customer_ID').agg(['first', 'last', 'mean', 'std']).pipe(flatten_columns)\n",
    "        \n",
    "        diff_cols_a = [f\"B_{i}\" for i in [11, 14, 17]] + [\"D_39\", \"D_131\"] + [f\"S_{i}\" for i in [16, 23]]\n",
    "        diff_cols_b = [\"P_2\", \"P_3\"]\n",
    "        diff_feat = dataset[diff_cols_a + diff_cols_b]\n",
    "        for a in diff_cols_a:\n",
    "            for b in diff_cols_b:\n",
    "                    diff_feat[f\"{a}-{b}\"] = diff_feat[a] - diff_feat[b]\n",
    "        diff_feat.drop(diff_cols_a + diff_cols_b, axis=1, inplace=True)\n",
    "        diff_feat = diff_feat.groupby(level='customer_ID').agg(['first', 'last', 'mean', 'std']).pipe(flatten_columns)\n",
    "\n",
    "        dataset = cudf.concat([cat_feat, num_feat, diff_feat], axis=1)\n",
    "        return dataset\n",
    "    \n",
    "    if feature_set == 4:\n",
    "        cat_cols = [\n",
    "            \"B_30\",\n",
    "            \"B_38\",\n",
    "            \"D_114\",\n",
    "            \"D_116\",\n",
    "            \"D_117\",\n",
    "            \"D_120\",\n",
    "            \"D_126\",\n",
    "            \"D_63\",\n",
    "            \"D_64\",\n",
    "            \"D_66\",\n",
    "            \"D_68\",\n",
    "        ]\n",
    "        cat_feat = dataset[cat_cols].groupby(level='customer_ID').agg(['count', 'last', 'nunique']).pipe(flatten_columns)\n",
    "\n",
    "        num_cols = [col for col in dataset.columns if col not in cat_cols + ['target']]\n",
    "        num_feat = dataset[num_cols].groupby(level='customer_ID').agg(['first', 'last', 'mean', 'std', 'min', 'max']).pipe(flatten_columns)\n",
    "        \n",
    "        diff_cols_a = [f\"B_{i}\" for i in [11, 14, 17]] + [\"D_39\", \"D_131\"] + [f\"S_{i}\" for i in [16, 23]]\n",
    "        diff_cols_b = [\"P_2\", \"P_3\"]\n",
    "        diff_feat = dataset[diff_cols_a + diff_cols_b]\n",
    "        for a in diff_cols_a:\n",
    "            for b in diff_cols_b:\n",
    "                    diff_feat[f\"{a}-{b}\"] = diff_feat[a] - diff_feat[b]\n",
    "        diff_feat.drop(diff_cols_a + diff_cols_b, axis=1, inplace=True)\n",
    "        diff_feat = diff_feat.groupby(level='customer_ID').agg(['first', 'last', 'mean', 'std', 'min', 'max']).pipe(flatten_columns)\n",
    "\n",
    "        dataset = cudf.concat([cat_feat, num_feat, diff_feat], axis=1)\n",
    "        return dataset\n",
    "\n",
    "    if feature_set == 5:\n",
    "        cat_cols = [\n",
    "            \"B_30\",\n",
    "            \"B_38\",\n",
    "            \"D_114\",\n",
    "            \"D_116\",\n",
    "            \"D_117\",\n",
    "            \"D_120\",\n",
    "            \"D_126\",\n",
    "            \"D_63\",\n",
    "            \"D_64\",\n",
    "            \"D_66\",\n",
    "            \"D_68\",\n",
    "        ]\n",
    "        cat_feat = dataset[cat_cols].groupby(level='customer_ID').agg(['count', 'last', 'nunique']).pipe(flatten_columns)\n",
    "\n",
    "        num_cols = [col for col in dataset.columns if col not in cat_cols + ['target']]\n",
    "        num_feat = dataset[num_cols].groupby(level='customer_ID').agg(['first', 'last', 'mean', 'std']).pipe(flatten_columns)\n",
    "        \n",
    "        for col in num_feat.columns:\n",
    "            if 'last' in col and col.replace('last', 'first') in num_feat.columns:\n",
    "                num_feat[col + '_lag_sub'] = num_feat[col] - num_feat[col.replace('last', 'first')]\n",
    "                num_feat[col + '_lag_div'] = num_feat[col] / num_feat[col.replace('last', 'first')]\n",
    "        \n",
    "        diff_cols_a = [f\"B_{i}\" for i in [11, 14, 17]] + [\"D_39\", \"D_131\"] + [f\"S_{i}\" for i in [16, 23]]\n",
    "        diff_cols_b = [\"P_2\", \"P_3\"]\n",
    "        diff_feat = dataset[diff_cols_a + diff_cols_b]\n",
    "        for a in diff_cols_a:\n",
    "            for b in diff_cols_b:\n",
    "                    diff_feat[f\"{a}-{b}\"] = diff_feat[a] - diff_feat[b]\n",
    "        diff_feat.drop(diff_cols_a + diff_cols_b, axis=1, inplace=True)\n",
    "        diff_feat = diff_feat.groupby(level='customer_ID').agg(['first', 'last', 'mean', 'std']).pipe(flatten_columns)\n",
    "\n",
    "        dataset = cudf.concat([cat_feat, num_feat, diff_feat], axis=1)\n",
    "        return dataset\n",
    "\n",
    "    if feature_set == 6:\n",
    "        cat_cols = [\n",
    "            \"B_30\",\n",
    "            \"B_38\",\n",
    "            \"D_114\",\n",
    "            \"D_116\",\n",
    "            \"D_117\",\n",
    "            \"D_120\",\n",
    "            \"D_126\",\n",
    "            \"D_63\",\n",
    "            \"D_64\",\n",
    "            \"D_66\",\n",
    "            \"D_68\",\n",
    "        ]\n",
    "        cat_feat = dataset[cat_cols].groupby(level='customer_ID').agg(['count', 'last', 'nunique']).pipe(flatten_columns)\n",
    "\n",
    "        num_cols = [col for col in dataset.columns if col not in cat_cols + ['target']]\n",
    "        num_feat = dataset[num_cols].groupby(level='customer_ID').agg(['first', 'last', 'mean', 'std']).pipe(flatten_columns)\n",
    "        \n",
    "        diff_cols_a = [f\"B_{i}\" for i in [11, 14, 17]] + [\"D_39\", \"D_131\"] + [f\"S_{i}\" for i in [16, 23]]\n",
    "        diff_cols_b = [\"P_2\", \"P_3\"]\n",
    "        diff_feat = dataset[diff_cols_a + diff_cols_b]\n",
    "        for a in diff_cols_a:\n",
    "            for b in diff_cols_b:\n",
    "                    diff_feat[f\"{a}-{b}\"] = diff_feat[a] - diff_feat[b]\n",
    "        diff_feat.drop(diff_cols_a + diff_cols_b, axis=1, inplace=True)\n",
    "        diff_feat = diff_feat.groupby(level='customer_ID').agg(['first', 'last', 'mean', 'std']).pipe(flatten_columns)\n",
    "\n",
    "        for col in num_feat.columns:\n",
    "            if 'last' in col:\n",
    "                num_feat[col + '_round2'] = num_feat[col].round(2)\n",
    "        \n",
    "        dataset = cudf.concat([cat_feat, num_feat, diff_feat], axis=1)\n",
    "        return dataset\n",
    "\n",
    "    if feature_set == 7:\n",
    "        cat_cols = [\n",
    "            \"B_30\",\n",
    "            \"B_38\",\n",
    "            \"D_114\",\n",
    "            \"D_116\",\n",
    "            \"D_117\",\n",
    "            \"D_120\",\n",
    "            \"D_126\",\n",
    "            \"D_63\",\n",
    "            \"D_64\",\n",
    "            \"D_66\",\n",
    "            \"D_68\",\n",
    "        ]\n",
    "        cat_feat = dataset[cat_cols].groupby(level='customer_ID').agg(['count', 'last', 'nunique']).pipe(flatten_columns)\n",
    "\n",
    "        num_cols = [col for col in dataset.columns if col not in cat_cols + ['target']]\n",
    "        num_feat = dataset[num_cols].groupby(level='customer_ID').agg(['first', 'last', 'mean', 'std']).pipe(flatten_columns)\n",
    "        \n",
    "        for col in num_cols:\n",
    "            num_feat[col + \"_sub_mean\"] = num_feat[col + \"_last\"] - num_feat[col + \"_mean\"]\n",
    "        \n",
    "        diff_cols_a = [f\"B_{i}\" for i in [11, 14, 17]] + [\"D_39\", \"D_131\"] + [f\"S_{i}\" for i in [16, 23]]\n",
    "        diff_cols_b = [\"P_2\", \"P_3\"]\n",
    "        diff_feat = dataset[diff_cols_a + diff_cols_b]\n",
    "        for a in diff_cols_a:\n",
    "            for b in diff_cols_b:\n",
    "                    diff_feat[f\"{a}-{b}\"] = diff_feat[a] - diff_feat[b]\n",
    "        diff_feat.drop(diff_cols_a + diff_cols_b, axis=1, inplace=True)\n",
    "        diff_feat = diff_feat.groupby(level='customer_ID').agg(['first', 'last', 'mean', 'std']).pipe(flatten_columns)\n",
    "\n",
    "        dataset = cudf.concat([cat_feat, num_feat, diff_feat], axis=1)\n",
    "        return dataset    \n",
    "    \n",
    "    \n",
    "def add_train_labels(train):\n",
    "    train_labels = cudf.read_csv('/kaggle/input/amex-default-prediction/train_labels.csv')\n",
    "\n",
    "    train_labels['customer_ID'] = train_labels['customer_ID'].str[-16:].str.hex_to_int().astype('int64')\n",
    "    train_labels.set_index('customer_ID', inplace=True)\n",
    "    \n",
    "    return cudf.merge(train, train_labels, how='inner', left_index=True, right_index=True).sort_index()\n",
    "\n",
    "\n",
    "def main(*, feature_set, num_rows):\n",
    "    train = cudf.read_parquet(\"../input/amex-data-integer-dtypes-parquet-format/train.parquet\", num_rows=num_rows)\n",
    "    train = preprocess(train)\n",
    "    train = engineer(train, feature_set)\n",
    "    train = add_train_labels(train)\n",
    "    \n",
    "    kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    train['fold'] = -1\n",
    "    for fold_ix, (train_ixs, valid_ixs) in enumerate(kfold.split(train, train['target'].to_array())):\n",
    "        train['fold'].iloc[valid_ixs] = fold_ix\n",
    "    \n",
    "    print(feature_set, train.shape)\n",
    "    train.to_parquet('train.pq')\n",
    "\n",
    "\n",
    "main(\n",
    "    feature_set = 7,\n",
    "    num_rows = None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f9a70cd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-26T20:04:12.851935Z",
     "iopub.status.busy": "2022-07-26T20:04:12.851283Z",
     "iopub.status.idle": "2022-07-26T20:32:04.247709Z",
     "shell.execute_reply": "2022-07-26T20:32:04.246682Z"
    },
    "papermill": {
     "duration": 1671.403537,
     "end_time": "2022-07-26T20:32:04.250337",
     "exception": false,
     "start_time": "2022-07-26T20:04:12.846800",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "974\n",
      "\u001b[34m########## Fold 0 ##########\u001b[0m\n",
      "[0]\ttrain-logloss:0.67366\ttrain-amex:0.71546\tvalid-logloss:0.67377\tvalid-amex:0.70876\n",
      "[100]\ttrain-logloss:0.24071\ttrain-amex:0.78390\tvalid-logloss:0.24700\tvalid-amex:0.76878\n",
      "[200]\ttrain-logloss:0.21649\ttrain-amex:0.79988\tvalid-logloss:0.22693\tvalid-amex:0.77900\n",
      "[300]\ttrain-logloss:0.20786\ttrain-amex:0.81188\tvalid-logloss:0.22237\tvalid-amex:0.78365\n",
      "[400]\ttrain-logloss:0.20226\ttrain-amex:0.82020\tvalid-logloss:0.22019\tvalid-amex:0.78675\n",
      "[500]\ttrain-logloss:0.19752\ttrain-amex:0.82805\tvalid-logloss:0.21893\tvalid-amex:0.78911\n",
      "[600]\ttrain-logloss:0.19323\ttrain-amex:0.83533\tvalid-logloss:0.21807\tvalid-amex:0.78982\n",
      "[700]\ttrain-logloss:0.18945\ttrain-amex:0.84174\tvalid-logloss:0.21750\tvalid-amex:0.79048\n",
      "[800]\ttrain-logloss:0.18564\ttrain-amex:0.84828\tvalid-logloss:0.21701\tvalid-amex:0.79045\n",
      "[900]\ttrain-logloss:0.18213\ttrain-amex:0.85435\tvalid-logloss:0.21667\tvalid-amex:0.79131\n",
      "[1000]\ttrain-logloss:0.17879\ttrain-amex:0.86036\tvalid-logloss:0.21640\tvalid-amex:0.79211\n",
      "[1100]\ttrain-logloss:0.17536\ttrain-amex:0.86632\tvalid-logloss:0.21622\tvalid-amex:0.79241\n",
      "[1200]\ttrain-logloss:0.17227\ttrain-amex:0.87198\tvalid-logloss:0.21608\tvalid-amex:0.79276\n",
      "[1300]\ttrain-logloss:0.16916\ttrain-amex:0.87800\tvalid-logloss:0.21597\tvalid-amex:0.79268\n",
      "[1400]\ttrain-logloss:0.16621\ttrain-amex:0.88310\tvalid-logloss:0.21587\tvalid-amex:0.79345\n",
      "[1500]\ttrain-logloss:0.16340\ttrain-amex:0.88823\tvalid-logloss:0.21583\tvalid-amex:0.79311\n",
      "[1600]\ttrain-logloss:0.16064\ttrain-amex:0.89299\tvalid-logloss:0.21576\tvalid-amex:0.79301\n",
      "[1700]\ttrain-logloss:0.15788\ttrain-amex:0.89767\tvalid-logloss:0.21569\tvalid-amex:0.79291\n",
      "[1800]\ttrain-logloss:0.15536\ttrain-amex:0.90256\tvalid-logloss:0.21561\tvalid-amex:0.79273\n",
      "[1900]\ttrain-logloss:0.15277\ttrain-amex:0.90688\tvalid-logloss:0.21559\tvalid-amex:0.79399\n",
      "[2000]\ttrain-logloss:0.15036\ttrain-amex:0.91083\tvalid-logloss:0.21557\tvalid-amex:0.79445\n",
      "[2100]\ttrain-logloss:0.14782\ttrain-amex:0.91536\tvalid-logloss:0.21553\tvalid-amex:0.79411\n",
      "[2200]\ttrain-logloss:0.14535\ttrain-amex:0.91942\tvalid-logloss:0.21554\tvalid-amex:0.79363\n",
      "[2300]\ttrain-logloss:0.14301\ttrain-amex:0.92351\tvalid-logloss:0.21553\tvalid-amex:0.79374\n",
      "[2400]\ttrain-logloss:0.14070\ttrain-amex:0.92694\tvalid-logloss:0.21553\tvalid-amex:0.79373\n",
      "[2500]\ttrain-logloss:0.13849\ttrain-amex:0.93065\tvalid-logloss:0.21556\tvalid-amex:0.79454\n",
      "[2600]\ttrain-logloss:0.13613\ttrain-amex:0.93451\tvalid-logloss:0.21558\tvalid-amex:0.79394\n",
      "[2700]\ttrain-logloss:0.13385\ttrain-amex:0.93812\tvalid-logloss:0.21563\tvalid-amex:0.79410\n",
      "[2800]\ttrain-logloss:0.13183\ttrain-amex:0.94096\tvalid-logloss:0.21566\tvalid-amex:0.79421\n",
      "[2900]\ttrain-logloss:0.12973\ttrain-amex:0.94421\tvalid-logloss:0.21572\tvalid-amex:0.79369\n",
      "[3000]\ttrain-logloss:0.12754\ttrain-amex:0.94733\tvalid-logloss:0.21571\tvalid-amex:0.79369\n",
      "[3032]\ttrain-logloss:0.12690\ttrain-amex:0.94818\tvalid-logloss:0.21575\tvalid-amex:0.79366\n",
      "Fold: 0.7948 CV\n",
      "\u001b[34m############################ \n",
      "\u001b[0m\n",
      "\u001b[34m########## Fold 1 ##########\u001b[0m\n",
      "[0]\ttrain-logloss:0.67366\ttrain-amex:0.70841\tvalid-logloss:0.67371\tvalid-amex:0.70545\n",
      "[100]\ttrain-logloss:0.24066\ttrain-amex:0.78441\tvalid-logloss:0.24719\tvalid-amex:0.76925\n",
      "[200]\ttrain-logloss:0.21654\ttrain-amex:0.79939\tvalid-logloss:0.22742\tvalid-amex:0.78072\n",
      "[300]\ttrain-logloss:0.20776\ttrain-amex:0.81130\tvalid-logloss:0.22298\tvalid-amex:0.78604\n",
      "[400]\ttrain-logloss:0.20176\ttrain-amex:0.82081\tvalid-logloss:0.22077\tvalid-amex:0.78871\n",
      "[500]\ttrain-logloss:0.19682\ttrain-amex:0.82895\tvalid-logloss:0.21948\tvalid-amex:0.78939\n",
      "[600]\ttrain-logloss:0.19252\ttrain-amex:0.83655\tvalid-logloss:0.21877\tvalid-amex:0.78958\n",
      "[700]\ttrain-logloss:0.18856\ttrain-amex:0.84308\tvalid-logloss:0.21818\tvalid-amex:0.79097\n",
      "[800]\ttrain-logloss:0.18464\ttrain-amex:0.85004\tvalid-logloss:0.21780\tvalid-amex:0.79088\n",
      "[900]\ttrain-logloss:0.18124\ttrain-amex:0.85612\tvalid-logloss:0.21750\tvalid-amex:0.79145\n",
      "[1000]\ttrain-logloss:0.17804\ttrain-amex:0.86149\tvalid-logloss:0.21722\tvalid-amex:0.79126\n",
      "[1100]\ttrain-logloss:0.17471\ttrain-amex:0.86776\tvalid-logloss:0.21704\tvalid-amex:0.79175\n",
      "[1200]\ttrain-logloss:0.17155\ttrain-amex:0.87364\tvalid-logloss:0.21689\tvalid-amex:0.79161\n",
      "[1300]\ttrain-logloss:0.16857\ttrain-amex:0.87883\tvalid-logloss:0.21677\tvalid-amex:0.79200\n",
      "[1400]\ttrain-logloss:0.16550\ttrain-amex:0.88416\tvalid-logloss:0.21666\tvalid-amex:0.79242\n",
      "[1500]\ttrain-logloss:0.16267\ttrain-amex:0.88928\tvalid-logloss:0.21661\tvalid-amex:0.79220\n",
      "[1600]\ttrain-logloss:0.15964\ttrain-amex:0.89469\tvalid-logloss:0.21657\tvalid-amex:0.79276\n",
      "[1700]\ttrain-logloss:0.15708\ttrain-amex:0.89903\tvalid-logloss:0.21655\tvalid-amex:0.79322\n",
      "[1800]\ttrain-logloss:0.15452\ttrain-amex:0.90344\tvalid-logloss:0.21651\tvalid-amex:0.79307\n",
      "[1900]\ttrain-logloss:0.15170\ttrain-amex:0.90831\tvalid-logloss:0.21644\tvalid-amex:0.79279\n",
      "[2000]\ttrain-logloss:0.14912\ttrain-amex:0.91264\tvalid-logloss:0.21644\tvalid-amex:0.79328\n",
      "[2100]\ttrain-logloss:0.14654\ttrain-amex:0.91711\tvalid-logloss:0.21644\tvalid-amex:0.79320\n",
      "[2200]\ttrain-logloss:0.14403\ttrain-amex:0.92132\tvalid-logloss:0.21649\tvalid-amex:0.79284\n",
      "[2300]\ttrain-logloss:0.14169\ttrain-amex:0.92508\tvalid-logloss:0.21651\tvalid-amex:0.79341\n",
      "[2400]\ttrain-logloss:0.13940\ttrain-amex:0.92910\tvalid-logloss:0.21655\tvalid-amex:0.79364\n",
      "[2500]\ttrain-logloss:0.13709\ttrain-amex:0.93265\tvalid-logloss:0.21650\tvalid-amex:0.79425\n",
      "[2600]\ttrain-logloss:0.13490\ttrain-amex:0.93629\tvalid-logloss:0.21658\tvalid-amex:0.79390\n",
      "[2700]\ttrain-logloss:0.13271\ttrain-amex:0.93946\tvalid-logloss:0.21658\tvalid-amex:0.79414\n",
      "[2800]\ttrain-logloss:0.13056\ttrain-amex:0.94268\tvalid-logloss:0.21665\tvalid-amex:0.79413\n",
      "[2900]\ttrain-logloss:0.12861\ttrain-amex:0.94538\tvalid-logloss:0.21666\tvalid-amex:0.79389\n",
      "[3000]\ttrain-logloss:0.12647\ttrain-amex:0.94828\tvalid-logloss:0.21671\tvalid-amex:0.79414\n",
      "[3100]\ttrain-logloss:0.12425\ttrain-amex:0.95154\tvalid-logloss:0.21677\tvalid-amex:0.79432\n",
      "[3172]\ttrain-logloss:0.12277\ttrain-amex:0.95364\tvalid-logloss:0.21683\tvalid-amex:0.79432\n",
      "Fold: 0.7945 CV\n",
      "\u001b[34m############################ \n",
      "\u001b[0m\n",
      "\u001b[34m########## Fold 2 ##########\u001b[0m\n",
      "[0]\ttrain-logloss:0.67364\ttrain-amex:0.71040\tvalid-logloss:0.67377\tvalid-amex:0.70416\n",
      "[100]\ttrain-logloss:0.24064\ttrain-amex:0.78375\tvalid-logloss:0.24732\tvalid-amex:0.76953\n",
      "[200]\ttrain-logloss:0.21642\ttrain-amex:0.79919\tvalid-logloss:0.22717\tvalid-amex:0.77998\n",
      "[300]\ttrain-logloss:0.20767\ttrain-amex:0.81215\tvalid-logloss:0.22263\tvalid-amex:0.78661\n",
      "[400]\ttrain-logloss:0.20173\ttrain-amex:0.82125\tvalid-logloss:0.22047\tvalid-amex:0.78847\n",
      "[500]\ttrain-logloss:0.19684\ttrain-amex:0.82905\tvalid-logloss:0.21921\tvalid-amex:0.78947\n",
      "[600]\ttrain-logloss:0.19255\ttrain-amex:0.83648\tvalid-logloss:0.21837\tvalid-amex:0.79126\n",
      "[700]\ttrain-logloss:0.18872\ttrain-amex:0.84330\tvalid-logloss:0.21780\tvalid-amex:0.79200\n",
      "[800]\ttrain-logloss:0.18511\ttrain-amex:0.84953\tvalid-logloss:0.21740\tvalid-amex:0.79234\n",
      "[900]\ttrain-logloss:0.18166\ttrain-amex:0.85550\tvalid-logloss:0.21704\tvalid-amex:0.79303\n",
      "[1000]\ttrain-logloss:0.17864\ttrain-amex:0.86084\tvalid-logloss:0.21680\tvalid-amex:0.79312\n",
      "[1100]\ttrain-logloss:0.17548\ttrain-amex:0.86633\tvalid-logloss:0.21662\tvalid-amex:0.79363\n",
      "[1200]\ttrain-logloss:0.17234\ttrain-amex:0.87200\tvalid-logloss:0.21640\tvalid-amex:0.79441\n",
      "[1300]\ttrain-logloss:0.16919\ttrain-amex:0.87755\tvalid-logloss:0.21631\tvalid-amex:0.79413\n",
      "[1400]\ttrain-logloss:0.16623\ttrain-amex:0.88281\tvalid-logloss:0.21622\tvalid-amex:0.79427\n",
      "[1500]\ttrain-logloss:0.16353\ttrain-amex:0.88783\tvalid-logloss:0.21617\tvalid-amex:0.79484\n",
      "[1600]\ttrain-logloss:0.16074\ttrain-amex:0.89282\tvalid-logloss:0.21617\tvalid-amex:0.79462\n",
      "[1700]\ttrain-logloss:0.15796\ttrain-amex:0.89791\tvalid-logloss:0.21609\tvalid-amex:0.79427\n",
      "[1800]\ttrain-logloss:0.15516\ttrain-amex:0.90227\tvalid-logloss:0.21607\tvalid-amex:0.79494\n",
      "[1900]\ttrain-logloss:0.15249\ttrain-amex:0.90659\tvalid-logloss:0.21600\tvalid-amex:0.79602\n",
      "[2000]\ttrain-logloss:0.14989\ttrain-amex:0.91120\tvalid-logloss:0.21600\tvalid-amex:0.79554\n",
      "[2100]\ttrain-logloss:0.14750\ttrain-amex:0.91524\tvalid-logloss:0.21605\tvalid-amex:0.79635\n",
      "[2200]\ttrain-logloss:0.14500\ttrain-amex:0.91956\tvalid-logloss:0.21612\tvalid-amex:0.79583\n",
      "[2300]\ttrain-logloss:0.14252\ttrain-amex:0.92384\tvalid-logloss:0.21609\tvalid-amex:0.79524\n",
      "[2400]\ttrain-logloss:0.14018\ttrain-amex:0.92767\tvalid-logloss:0.21614\tvalid-amex:0.79567\n",
      "[2500]\ttrain-logloss:0.13780\ttrain-amex:0.93168\tvalid-logloss:0.21620\tvalid-amex:0.79551\n",
      "[2585]\ttrain-logloss:0.13589\ttrain-amex:0.93470\tvalid-logloss:0.21623\tvalid-amex:0.79581\n",
      "Fold: 0.7965 CV\n",
      "\u001b[34m############################ \n",
      "\u001b[0m\n",
      "\u001b[34m########## Fold 3 ##########\u001b[0m\n",
      "[0]\ttrain-logloss:0.67363\ttrain-amex:0.72361\tvalid-logloss:0.67375\tvalid-amex:0.70959\n",
      "[100]\ttrain-logloss:0.24050\ttrain-amex:0.78347\tvalid-logloss:0.24774\tvalid-amex:0.76945\n",
      "[200]\ttrain-logloss:0.21615\ttrain-amex:0.79905\tvalid-logloss:0.22806\tvalid-amex:0.77936\n",
      "[300]\ttrain-logloss:0.20736\ttrain-amex:0.81204\tvalid-logloss:0.22365\tvalid-amex:0.78458\n",
      "[400]\ttrain-logloss:0.20135\ttrain-amex:0.82135\tvalid-logloss:0.22147\tvalid-amex:0.78681\n",
      "[500]\ttrain-logloss:0.19643\ttrain-amex:0.82948\tvalid-logloss:0.22018\tvalid-amex:0.78880\n",
      "[600]\ttrain-logloss:0.19235\ttrain-amex:0.83631\tvalid-logloss:0.21943\tvalid-amex:0.79064\n",
      "[700]\ttrain-logloss:0.18850\ttrain-amex:0.84303\tvalid-logloss:0.21889\tvalid-amex:0.79212\n",
      "[800]\ttrain-logloss:0.18465\ttrain-amex:0.84976\tvalid-logloss:0.21846\tvalid-amex:0.79232\n",
      "[900]\ttrain-logloss:0.18107\ttrain-amex:0.85580\tvalid-logloss:0.21818\tvalid-amex:0.79243\n",
      "[1000]\ttrain-logloss:0.17782\ttrain-amex:0.86152\tvalid-logloss:0.21791\tvalid-amex:0.79360\n",
      "[1100]\ttrain-logloss:0.17466\ttrain-amex:0.86741\tvalid-logloss:0.21771\tvalid-amex:0.79366\n",
      "[1200]\ttrain-logloss:0.17155\ttrain-amex:0.87300\tvalid-logloss:0.21752\tvalid-amex:0.79413\n",
      "[1300]\ttrain-logloss:0.16843\ttrain-amex:0.87868\tvalid-logloss:0.21743\tvalid-amex:0.79441\n",
      "[1400]\ttrain-logloss:0.16542\ttrain-amex:0.88395\tvalid-logloss:0.21731\tvalid-amex:0.79451\n",
      "[1500]\ttrain-logloss:0.16279\ttrain-amex:0.88839\tvalid-logloss:0.21723\tvalid-amex:0.79482\n",
      "[1600]\ttrain-logloss:0.15997\ttrain-amex:0.89348\tvalid-logloss:0.21720\tvalid-amex:0.79446\n",
      "[1700]\ttrain-logloss:0.15722\ttrain-amex:0.89855\tvalid-logloss:0.21717\tvalid-amex:0.79416\n",
      "[1800]\ttrain-logloss:0.15452\ttrain-amex:0.90322\tvalid-logloss:0.21712\tvalid-amex:0.79437\n",
      "[1900]\ttrain-logloss:0.15197\ttrain-amex:0.90782\tvalid-logloss:0.21711\tvalid-amex:0.79511\n",
      "[2000]\ttrain-logloss:0.14966\ttrain-amex:0.91174\tvalid-logloss:0.21712\tvalid-amex:0.79535\n",
      "[2100]\ttrain-logloss:0.14717\ttrain-amex:0.91606\tvalid-logloss:0.21708\tvalid-amex:0.79524\n",
      "[2200]\ttrain-logloss:0.14469\ttrain-amex:0.92013\tvalid-logloss:0.21712\tvalid-amex:0.79562\n",
      "[2300]\ttrain-logloss:0.14218\ttrain-amex:0.92434\tvalid-logloss:0.21710\tvalid-amex:0.79570\n",
      "[2400]\ttrain-logloss:0.13979\ttrain-amex:0.92818\tvalid-logloss:0.21712\tvalid-amex:0.79578\n",
      "[2500]\ttrain-logloss:0.13751\ttrain-amex:0.93161\tvalid-logloss:0.21714\tvalid-amex:0.79515\n",
      "[2600]\ttrain-logloss:0.13517\ttrain-amex:0.93567\tvalid-logloss:0.21717\tvalid-amex:0.79581\n",
      "[2700]\ttrain-logloss:0.13303\ttrain-amex:0.93873\tvalid-logloss:0.21721\tvalid-amex:0.79532\n",
      "[2800]\ttrain-logloss:0.13088\ttrain-amex:0.94213\tvalid-logloss:0.21724\tvalid-amex:0.79490\n",
      "[2853]\ttrain-logloss:0.12980\ttrain-amex:0.94382\tvalid-logloss:0.21726\tvalid-amex:0.79490\n",
      "Fold: 0.7959 CV\n",
      "\u001b[34m############################ \n",
      "\u001b[0m\n",
      "\u001b[34m########## Fold 4 ##########\u001b[0m\n",
      "[0]\ttrain-logloss:0.67367\ttrain-amex:0.71482\tvalid-logloss:0.67366\tvalid-amex:0.71518\n",
      "[100]\ttrain-logloss:0.24122\ttrain-amex:0.78371\tvalid-logloss:0.24543\tvalid-amex:0.77333\n",
      "[200]\ttrain-logloss:0.21677\ttrain-amex:0.79843\tvalid-logloss:0.22531\tvalid-amex:0.78211\n",
      "[300]\ttrain-logloss:0.20797\ttrain-amex:0.81089\tvalid-logloss:0.22094\tvalid-amex:0.78812\n",
      "[400]\ttrain-logloss:0.20190\ttrain-amex:0.82049\tvalid-logloss:0.21891\tvalid-amex:0.79103\n",
      "[500]\ttrain-logloss:0.19720\ttrain-amex:0.82820\tvalid-logloss:0.21771\tvalid-amex:0.79184\n",
      "[600]\ttrain-logloss:0.19293\ttrain-amex:0.83504\tvalid-logloss:0.21695\tvalid-amex:0.79261\n",
      "[700]\ttrain-logloss:0.18899\ttrain-amex:0.84166\tvalid-logloss:0.21632\tvalid-amex:0.79319\n",
      "[800]\ttrain-logloss:0.18531\ttrain-amex:0.84795\tvalid-logloss:0.21586\tvalid-amex:0.79360\n",
      "[900]\ttrain-logloss:0.18196\ttrain-amex:0.85399\tvalid-logloss:0.21553\tvalid-amex:0.79393\n",
      "[1000]\ttrain-logloss:0.17860\ttrain-amex:0.86020\tvalid-logloss:0.21525\tvalid-amex:0.79462\n",
      "[1100]\ttrain-logloss:0.17533\ttrain-amex:0.86606\tvalid-logloss:0.21503\tvalid-amex:0.79460\n",
      "[1200]\ttrain-logloss:0.17226\ttrain-amex:0.87189\tvalid-logloss:0.21491\tvalid-amex:0.79508\n",
      "[1300]\ttrain-logloss:0.16928\ttrain-amex:0.87682\tvalid-logloss:0.21481\tvalid-amex:0.79516\n",
      "[1400]\ttrain-logloss:0.16638\ttrain-amex:0.88200\tvalid-logloss:0.21471\tvalid-amex:0.79534\n",
      "[1500]\ttrain-logloss:0.16345\ttrain-amex:0.88726\tvalid-logloss:0.21465\tvalid-amex:0.79612\n",
      "[1600]\ttrain-logloss:0.16058\ttrain-amex:0.89213\tvalid-logloss:0.21457\tvalid-amex:0.79607\n",
      "[1700]\ttrain-logloss:0.15780\ttrain-amex:0.89716\tvalid-logloss:0.21451\tvalid-amex:0.79637\n",
      "[1800]\ttrain-logloss:0.15517\ttrain-amex:0.90187\tvalid-logloss:0.21442\tvalid-amex:0.79645\n",
      "[1900]\ttrain-logloss:0.15264\ttrain-amex:0.90625\tvalid-logloss:0.21436\tvalid-amex:0.79705\n",
      "[2000]\ttrain-logloss:0.15016\ttrain-amex:0.91067\tvalid-logloss:0.21437\tvalid-amex:0.79650\n",
      "[2100]\ttrain-logloss:0.14754\ttrain-amex:0.91533\tvalid-logloss:0.21436\tvalid-amex:0.79651\n",
      "[2200]\ttrain-logloss:0.14512\ttrain-amex:0.91933\tvalid-logloss:0.21431\tvalid-amex:0.79662\n",
      "[2300]\ttrain-logloss:0.14254\ttrain-amex:0.92391\tvalid-logloss:0.21432\tvalid-amex:0.79702\n",
      "[2400]\ttrain-logloss:0.14024\ttrain-amex:0.92758\tvalid-logloss:0.21433\tvalid-amex:0.79663\n",
      "[2500]\ttrain-logloss:0.13788\ttrain-amex:0.93141\tvalid-logloss:0.21433\tvalid-amex:0.79683\n",
      "[2600]\ttrain-logloss:0.13555\ttrain-amex:0.93521\tvalid-logloss:0.21437\tvalid-amex:0.79659\n",
      "[2700]\ttrain-logloss:0.13342\ttrain-amex:0.93849\tvalid-logloss:0.21438\tvalid-amex:0.79639\n",
      "[2800]\ttrain-logloss:0.13132\ttrain-amex:0.94166\tvalid-logloss:0.21441\tvalid-amex:0.79622\n",
      "[2840]\ttrain-logloss:0.13049\ttrain-amex:0.94271\tvalid-logloss:0.21443\tvalid-amex:0.79641\n",
      "Fold: 0.7976 CV\n",
      "\u001b[34m############################ \n",
      "\u001b[0m\n",
      "\u001b[1mResults: 0.7956 CV\u001b[0m\n",
      "CPU times: user 26min 53s, sys: 52.4 s, total: 27min 45s\n",
      "Wall time: 27min 51s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%reset -f\n",
    "import gc; gc.collect()\n",
    "import cudf\n",
    "from colorama import Style, Fore\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "import cupy as cp\n",
    "\n",
    "\n",
    "def check_input(arr):\n",
    "    if type(arr) is pd.DataFrame:\n",
    "        arr = arr[arr.columns[0]]\n",
    "        \n",
    "    if type(arr) is pd.Series:\n",
    "        arr = arr.values\n",
    "        \n",
    "    if len(arr.shape) > 1:\n",
    "        arr = arr[:, 0]\n",
    "        \n",
    "    return arr\n",
    "\n",
    "\n",
    "def gini(cs_0, cs_1, sum_0, sum_1):\n",
    "    auc_ = (cs_0 - sum_0 / 2) * sum_1\n",
    "    tot = cs_0[-1] * cs_1[-1]\n",
    "\n",
    "    return 2 * float(auc_.sum() / tot) - 1\n",
    "\n",
    "\n",
    "def recall_at4(cs_0, cs_1, sum_1):\n",
    "    cs_tot = cs_0 + cs_1\n",
    "    th = cs_tot[-1] * 0.96\n",
    "    \n",
    "    return float(sum_1[cs_tot >= th].sum() / cs_1[-1])\n",
    "\n",
    "\n",
    "def amex_metric_cupy(y_true, y_pred):\n",
    "    y_true = cp.asarray(check_input(y_true))\n",
    "    y_pred = cp.asarray(check_input(y_pred))\n",
    "    \n",
    "    unique = cp.unique(y_pred)\n",
    "    rank = cp.searchsorted(unique, y_pred)\n",
    "    \n",
    "    sum_1 = cp.zeros_like(unique, dtype=cp.float64)\n",
    "    sum_1.scatter_add(rank, y_true)\n",
    "    \n",
    "    sum_0 = cp.zeros_like(unique, dtype=cp.float64)\n",
    "    sum_0.scatter_add(rank, 1 - y_true)\n",
    "    sum_0 *= 20\n",
    "    \n",
    "    cs_0, cs_1 = sum_0.cumsum(), sum_1.cumsum()\n",
    "    \n",
    "    g = gini(cs_0, cs_1, sum_0, sum_1)\n",
    "    d = recall_at4(cs_0, cs_1, sum_1)\n",
    "    \n",
    "    return (g + d) / 2\n",
    "\n",
    "\n",
    "def xgb_amex(y_pred, dmatrix):\n",
    "    return \"amex\", amex_metric_cupy(dmatrix.get_label(), y_pred)\n",
    "\n",
    "\n",
    "def amex_metric(y_true: pd.DataFrame, y_pred: pd.DataFrame) -> float:\n",
    "    def top_four_percent_captured(y_true: pd.DataFrame, y_pred: pd.DataFrame) -> float:\n",
    "        df = pd.concat([y_true, y_pred], axis=\"columns\").sort_values(\n",
    "            \"prediction\", ascending=False\n",
    "        )\n",
    "        df[\"weight\"] = df[\"target\"].apply(lambda x: 20 if x == 0 else 1)\n",
    "        four_pct_cutoff = int(0.04 * df[\"weight\"].sum())\n",
    "        df[\"weight_cumsum\"] = df[\"weight\"].cumsum()\n",
    "        df_cutoff = df.loc[df[\"weight_cumsum\"] <= four_pct_cutoff]\n",
    "        return (df_cutoff[\"target\"] == 1).sum() / (df[\"target\"] == 1).sum()\n",
    "\n",
    "    def weighted_gini(y_true: pd.DataFrame, y_pred: pd.DataFrame) -> float:\n",
    "        df = pd.concat([y_true, y_pred], axis=\"columns\").sort_values(\n",
    "            \"prediction\", ascending=False\n",
    "        )\n",
    "        df[\"weight\"] = df[\"target\"].apply(lambda x: 20 if x == 0 else 1)\n",
    "        df[\"random\"] = (df[\"weight\"] / df[\"weight\"].sum()).cumsum()\n",
    "        total_pos = (df[\"target\"] * df[\"weight\"]).sum()\n",
    "        df[\"cum_pos_found\"] = (df[\"target\"] * df[\"weight\"]).cumsum()\n",
    "        df[\"lorentz\"] = df[\"cum_pos_found\"] / total_pos\n",
    "        df[\"gini\"] = (df[\"lorentz\"] - df[\"random\"]) * df[\"weight\"]\n",
    "        return df[\"gini\"].sum()\n",
    "\n",
    "    def normalized_weighted_gini(y_true: pd.DataFrame, y_pred: pd.DataFrame) -> float:\n",
    "        y_true_pred = y_true.rename(columns={\"target\": \"prediction\"})\n",
    "        return weighted_gini(y_true, y_pred) / weighted_gini(y_true, y_true_pred)\n",
    "\n",
    "    g = normalized_weighted_gini(y_true, y_pred)\n",
    "    d = top_four_percent_captured(y_true, y_pred)\n",
    "\n",
    "    return 0.5 * (g + d)\n",
    "\n",
    "\n",
    "def main(*, xgb_parameters=None, num_rows=None):\n",
    "    folds = cudf.read_parquet(\"train.pq\", num_rows=num_rows)\n",
    "    \n",
    "    features = [col for col in folds.columns if col not in ['target', 'fold']]\n",
    "    print(len(features))\n",
    "    \n",
    "    predictions = []\n",
    "    \n",
    "    for fold_ix in range(5):\n",
    "        print(Fore.BLUE + \"#\" * 10, f\"Fold {fold_ix}\", \"#\" * 10 + Style.RESET_ALL)\n",
    "        \n",
    "        train = folds[folds.fold != fold_ix]\n",
    "        valid = folds[folds.fold == fold_ix]\n",
    "        \n",
    "        dtrain = xgb.DMatrix(data=train[features], label=train['target'])\n",
    "        dvalid = xgb.DMatrix(data=valid[features], label=valid['target'])\n",
    "\n",
    "        model = xgb.train(\n",
    "            xgb_parameters,\n",
    "            dtrain=dtrain,\n",
    "            num_boost_round=9999,\n",
    "\n",
    "            evals=[(dtrain, \"train\"), (dvalid, \"valid\")],\n",
    "            early_stopping_rounds=500,\n",
    "            \n",
    "            custom_metric=xgb_amex,\n",
    "            maximize=True,\n",
    "\n",
    "            verbose_eval=100\n",
    "        )\n",
    "        \n",
    "        model.save_model(f\"xgb_fold{fold_ix}_seed{xgb_parameters['random_state']}.xgb\")\n",
    "        \n",
    "        prediction = pd.DataFrame({\n",
    "            \"prediction\": model.predict(dvalid, iteration_range=(0, model.best_iteration + 1)),\n",
    "            \"target\": valid['target'].to_array()\n",
    "        })\n",
    "        \n",
    "        print(f\"Fold: {amex_metric(prediction[['target']], prediction[['prediction']]):.4f} CV\")\n",
    "        predictions.append(prediction)\n",
    "\n",
    "        del dtrain, dvalid, model\n",
    "        gc.collect()\n",
    "    \n",
    "        print(Fore.BLUE + \"#\" * 28, \"\\n\" + Style.RESET_ALL)\n",
    "    \n",
    "    prediction = pd.concat(predictions)\n",
    "    print(Style.BRIGHT + f\"Results: {amex_metric(prediction[['target']], prediction[['prediction']]):.4f} CV\" + Style.RESET_ALL)\n",
    "\n",
    "main(\n",
    "    xgb_parameters={\n",
    "        'max_depth': 7,\n",
    "        'eta': 0.03,\n",
    "\n",
    "        'subsample': 0.88,\n",
    "        'colsample_bytree': 0.5,\n",
    "        \n",
    "        'objective': 'binary:logistic',\n",
    "        \n",
    "        'tree_method': 'gpu_hist',\n",
    "        'predictor': 'gpu_predictor',\n",
    "        \n",
    "        'random_state': 42,\n",
    "        \n",
    "        'gamma': 1.5,\n",
    "        'min_child_weight': 8,\n",
    "        'lambda': 70,\n",
    "    },\n",
    "    num_rows = None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4bd9b432",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-26T20:32:04.277042Z",
     "iopub.status.busy": "2022-07-26T20:32:04.275445Z",
     "iopub.status.idle": "2022-07-26T20:32:05.100088Z",
     "shell.execute_reply": "2022-07-26T20:32:05.098744Z"
    },
    "papermill": {
     "duration": 0.840165,
     "end_time": "2022-07-26T20:32:05.102684",
     "exception": false,
     "start_time": "2022-07-26T20:32:04.262519",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!rm train.pq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7ab39dc7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-26T20:32:05.129628Z",
     "iopub.status.busy": "2022-07-26T20:32:05.129278Z",
     "iopub.status.idle": "2022-07-26T20:32:05.638791Z",
     "shell.execute_reply": "2022-07-26T20:32:05.637928Z"
    },
    "papermill": {
     "duration": 0.524974,
     "end_time": "2022-07-26T20:32:05.640880",
     "exception": false,
     "start_time": "2022-07-26T20:32:05.115906",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 488 ms, sys: 11.1 ms, total: 499 ms\n",
      "Wall time: 497 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "%reset -f\n",
    "import gc; gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "adf4ebc4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-26T20:32:05.667940Z",
     "iopub.status.busy": "2022-07-26T20:32:05.666540Z",
     "iopub.status.idle": "2022-07-26T20:39:19.020760Z",
     "shell.execute_reply": "2022-07-26T20:39:19.019754Z"
    },
    "papermill": {
     "duration": 433.383069,
     "end_time": "2022-07-26T20:39:19.036083",
     "exception": false,
     "start_time": "2022-07-26T20:32:05.653014",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#########################\n",
      "### 0\n",
      "#########################\n",
      "#########################\n",
      "### 1\n",
      "#########################\n",
      "#########################\n",
      "### 2\n",
      "#########################\n",
      "#########################\n",
      "### 3\n",
      "#########################\n",
      "#########################\n",
      "### 4\n",
      "#########################\n",
      "#########################\n",
      "### 5\n",
      "#########################\n",
      "#########################\n",
      "### 6\n",
      "#########################\n",
      "#########################\n",
      "### 7\n",
      "#########################\n",
      "#########################\n",
      "### 8\n",
      "#########################\n",
      "#########################\n",
      "### 9\n",
      "#########################\n",
      "Submission file shape is (924621, 2)\n",
      "CPU times: user 11min 37s, sys: 11.9 s, total: 11min 49s\n",
      "Wall time: 7min 13s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%reset -f\n",
    "import gc; gc.collect()\n",
    "import cudf\n",
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def flatten_columns(df):\n",
    "    df.columns = [\"_\".join(column) for column in df.columns]\n",
    "    return df\n",
    "\n",
    "\n",
    "def preprocess(dataset):\n",
    "    dataset['customer_ID'] = dataset['customer_ID'].str[-16:].str.hex_to_int().astype('int64')\n",
    "    dataset['S_2'] = cudf.to_datetime(dataset['S_2'])\n",
    "    dataset.set_index(['customer_ID', 'S_2'], inplace=True)\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "\n",
    "def engineer(dataset, feature_set):\n",
    "    if feature_set == 0:\n",
    "        dataset = dataset.groupby(level='customer_ID').last()\n",
    "        return dataset\n",
    "    \n",
    "    if feature_set == 1:\n",
    "        cat_cols = [\n",
    "            \"B_30\",\n",
    "            \"B_38\",\n",
    "            \"D_114\",\n",
    "            \"D_116\",\n",
    "            \"D_117\",\n",
    "            \"D_120\",\n",
    "            \"D_126\",\n",
    "            \"D_63\",\n",
    "            \"D_64\",\n",
    "            \"D_66\",\n",
    "            \"D_68\",\n",
    "        ]\n",
    "        cat_feat = dataset[cat_cols].groupby(level='customer_ID').agg(['last']).pipe(flatten_columns)\n",
    "\n",
    "        num_cols = [col for col in dataset.columns if col not in cat_cols + ['target']]\n",
    "        num_feat = dataset[num_cols].groupby(level='customer_ID').agg(['first', 'last', 'mean', 'std']).pipe(flatten_columns)\n",
    "\n",
    "        dataset = cudf.concat([cat_feat, num_feat], axis=1)\n",
    "        return dataset\n",
    "    \n",
    "    if feature_set == 2:\n",
    "        cat_cols = [\n",
    "            \"B_30\",\n",
    "            \"B_38\",\n",
    "            \"D_114\",\n",
    "            \"D_116\",\n",
    "            \"D_117\",\n",
    "            \"D_120\",\n",
    "            \"D_126\",\n",
    "            \"D_63\",\n",
    "            \"D_64\",\n",
    "            \"D_66\",\n",
    "            \"D_68\",\n",
    "        ]\n",
    "        cat_feat = dataset[cat_cols].groupby(level='customer_ID').agg(['last']).pipe(flatten_columns)\n",
    "\n",
    "        num_cols = [col for col in dataset.columns if col not in cat_cols + ['target']]\n",
    "        num_feat = dataset[num_cols].groupby(level='customer_ID').agg(['first', 'last', 'mean', 'std']).pipe(flatten_columns)\n",
    "        \n",
    "        diff_cols_a = [f\"B_{i}\" for i in [11, 14, 17]] + [\"D_39\", \"D_131\"] + [f\"S_{i}\" for i in [16, 23]]\n",
    "        diff_cols_b = [\"P_2\", \"P_3\"]\n",
    "        diff_feat = dataset[diff_cols_a + diff_cols_b]\n",
    "        for a in diff_cols_a:\n",
    "            for b in diff_cols_b:\n",
    "                    diff_feat[f\"{a}-{b}\"] = diff_feat[a] - diff_feat[b]\n",
    "        diff_feat.drop(diff_cols_a + diff_cols_b, axis=1, inplace=True)\n",
    "        diff_feat = diff_feat.groupby(level='customer_ID').agg(['first', 'last', 'mean', 'std']).pipe(flatten_columns)\n",
    "\n",
    "        dataset = cudf.concat([cat_feat, num_feat, diff_feat], axis=1)\n",
    "        return dataset\n",
    "    \n",
    "    if feature_set == 3:\n",
    "        cat_cols = [\n",
    "            \"B_30\",\n",
    "            \"B_38\",\n",
    "            \"D_114\",\n",
    "            \"D_116\",\n",
    "            \"D_117\",\n",
    "            \"D_120\",\n",
    "            \"D_126\",\n",
    "            \"D_63\",\n",
    "            \"D_64\",\n",
    "            \"D_66\",\n",
    "            \"D_68\",\n",
    "        ]\n",
    "        cat_feat = dataset[cat_cols].groupby(level='customer_ID').agg(['count', 'last', 'nunique']).pipe(flatten_columns)\n",
    "\n",
    "        num_cols = [col for col in dataset.columns if col not in cat_cols + ['target']]\n",
    "        num_feat = dataset[num_cols].groupby(level='customer_ID').agg(['first', 'last', 'mean', 'std']).pipe(flatten_columns)\n",
    "        \n",
    "        diff_cols_a = [f\"B_{i}\" for i in [11, 14, 17]] + [\"D_39\", \"D_131\"] + [f\"S_{i}\" for i in [16, 23]]\n",
    "        diff_cols_b = [\"P_2\", \"P_3\"]\n",
    "        diff_feat = dataset[diff_cols_a + diff_cols_b]\n",
    "        for a in diff_cols_a:\n",
    "            for b in diff_cols_b:\n",
    "                    diff_feat[f\"{a}-{b}\"] = diff_feat[a] - diff_feat[b]\n",
    "        diff_feat.drop(diff_cols_a + diff_cols_b, axis=1, inplace=True)\n",
    "        diff_feat = diff_feat.groupby(level='customer_ID').agg(['first', 'last', 'mean', 'std']).pipe(flatten_columns)\n",
    "\n",
    "        dataset = cudf.concat([cat_feat, num_feat, diff_feat], axis=1)\n",
    "        return dataset\n",
    "    \n",
    "    if feature_set == 4:\n",
    "        cat_cols = [\n",
    "            \"B_30\",\n",
    "            \"B_38\",\n",
    "            \"D_114\",\n",
    "            \"D_116\",\n",
    "            \"D_117\",\n",
    "            \"D_120\",\n",
    "            \"D_126\",\n",
    "            \"D_63\",\n",
    "            \"D_64\",\n",
    "            \"D_66\",\n",
    "            \"D_68\",\n",
    "        ]\n",
    "        cat_feat = dataset[cat_cols].groupby(level='customer_ID').agg(['count', 'last', 'nunique']).pipe(flatten_columns)\n",
    "\n",
    "        num_cols = [col for col in dataset.columns if col not in cat_cols + ['target']]\n",
    "        num_feat = dataset[num_cols].groupby(level='customer_ID').agg(['first', 'last', 'mean', 'std', 'min', 'max']).pipe(flatten_columns)\n",
    "        \n",
    "        diff_cols_a = [f\"B_{i}\" for i in [11, 14, 17]] + [\"D_39\", \"D_131\"] + [f\"S_{i}\" for i in [16, 23]]\n",
    "        diff_cols_b = [\"P_2\", \"P_3\"]\n",
    "        diff_feat = dataset[diff_cols_a + diff_cols_b]\n",
    "        for a in diff_cols_a:\n",
    "            for b in diff_cols_b:\n",
    "                    diff_feat[f\"{a}-{b}\"] = diff_feat[a] - diff_feat[b]\n",
    "        diff_feat.drop(diff_cols_a + diff_cols_b, axis=1, inplace=True)\n",
    "        diff_feat = diff_feat.groupby(level='customer_ID').agg(['first', 'last', 'mean', 'std', 'min', 'max']).pipe(flatten_columns)\n",
    "\n",
    "        dataset = cudf.concat([cat_feat, num_feat, diff_feat], axis=1)\n",
    "        return dataset\n",
    "\n",
    "    if feature_set == 5:\n",
    "        cat_cols = [\n",
    "            \"B_30\",\n",
    "            \"B_38\",\n",
    "            \"D_114\",\n",
    "            \"D_116\",\n",
    "            \"D_117\",\n",
    "            \"D_120\",\n",
    "            \"D_126\",\n",
    "            \"D_63\",\n",
    "            \"D_64\",\n",
    "            \"D_66\",\n",
    "            \"D_68\",\n",
    "        ]\n",
    "        cat_feat = dataset[cat_cols].groupby(level='customer_ID').agg(['count', 'last', 'nunique']).pipe(flatten_columns)\n",
    "\n",
    "        num_cols = [col for col in dataset.columns if col not in cat_cols + ['target']]\n",
    "        num_feat = dataset[num_cols].groupby(level='customer_ID').agg(['first', 'last', 'mean', 'std']).pipe(flatten_columns)\n",
    "        \n",
    "        for col in num_feat.columns:\n",
    "            if 'last' in col and col.replace('last', 'first') in num_feat.columns:\n",
    "                num_feat[col + '_lag_sub'] = num_feat[col] - num_feat[col.replace('last', 'first')]\n",
    "                num_feat[col + '_lag_div'] = num_feat[col] / num_feat[col.replace('last', 'first')]\n",
    "        \n",
    "        diff_cols_a = [f\"B_{i}\" for i in [11, 14, 17]] + [\"D_39\", \"D_131\"] + [f\"S_{i}\" for i in [16, 23]]\n",
    "        diff_cols_b = [\"P_2\", \"P_3\"]\n",
    "        diff_feat = dataset[diff_cols_a + diff_cols_b]\n",
    "        for a in diff_cols_a:\n",
    "            for b in diff_cols_b:\n",
    "                    diff_feat[f\"{a}-{b}\"] = diff_feat[a] - diff_feat[b]\n",
    "        diff_feat.drop(diff_cols_a + diff_cols_b, axis=1, inplace=True)\n",
    "        diff_feat = diff_feat.groupby(level='customer_ID').agg(['first', 'last', 'mean', 'std']).pipe(flatten_columns)\n",
    "\n",
    "        dataset = cudf.concat([cat_feat, num_feat, diff_feat], axis=1)\n",
    "        return dataset\n",
    "\n",
    "    if feature_set == 6:\n",
    "        cat_cols = [\n",
    "            \"B_30\",\n",
    "            \"B_38\",\n",
    "            \"D_114\",\n",
    "            \"D_116\",\n",
    "            \"D_117\",\n",
    "            \"D_120\",\n",
    "            \"D_126\",\n",
    "            \"D_63\",\n",
    "            \"D_64\",\n",
    "            \"D_66\",\n",
    "            \"D_68\",\n",
    "        ]\n",
    "        cat_feat = dataset[cat_cols].groupby(level='customer_ID').agg(['count', 'last', 'nunique']).pipe(flatten_columns)\n",
    "\n",
    "        num_cols = [col for col in dataset.columns if col not in cat_cols + ['target']]\n",
    "        num_feat = dataset[num_cols].groupby(level='customer_ID').agg(['first', 'last', 'mean', 'std']).pipe(flatten_columns)\n",
    "        \n",
    "        diff_cols_a = [f\"B_{i}\" for i in [11, 14, 17]] + [\"D_39\", \"D_131\"] + [f\"S_{i}\" for i in [16, 23]]\n",
    "        diff_cols_b = [\"P_2\", \"P_3\"]\n",
    "        diff_feat = dataset[diff_cols_a + diff_cols_b]\n",
    "        for a in diff_cols_a:\n",
    "            for b in diff_cols_b:\n",
    "                    diff_feat[f\"{a}-{b}\"] = diff_feat[a] - diff_feat[b]\n",
    "        diff_feat.drop(diff_cols_a + diff_cols_b, axis=1, inplace=True)\n",
    "        diff_feat = diff_feat.groupby(level='customer_ID').agg(['first', 'last', 'mean', 'std']).pipe(flatten_columns)\n",
    "\n",
    "        for col in num_feat.columns:\n",
    "            if 'last' in col:\n",
    "                num_feat[col + '_round2'] = num_feat[col].round(2)\n",
    "        \n",
    "        dataset = cudf.concat([cat_feat, num_feat, diff_feat], axis=1)\n",
    "        return dataset\n",
    "\n",
    "    if feature_set == 7:\n",
    "        cat_cols = [\n",
    "            \"B_30\",\n",
    "            \"B_38\",\n",
    "            \"D_114\",\n",
    "            \"D_116\",\n",
    "            \"D_117\",\n",
    "            \"D_120\",\n",
    "            \"D_126\",\n",
    "            \"D_63\",\n",
    "            \"D_64\",\n",
    "            \"D_66\",\n",
    "            \"D_68\",\n",
    "        ]\n",
    "        cat_feat = dataset[cat_cols].groupby(level='customer_ID').agg(['count', 'last', 'nunique']).pipe(flatten_columns)\n",
    "\n",
    "        num_cols = [col for col in dataset.columns if col not in cat_cols + ['target']]\n",
    "        num_feat = dataset[num_cols].groupby(level='customer_ID').agg(['first', 'last', 'mean', 'std']).pipe(flatten_columns)\n",
    "        \n",
    "        for col in num_cols:\n",
    "            num_feat[col + \"_sub_mean\"] = num_feat[col + \"_last\"] - num_feat[col + \"_mean\"]\n",
    "        \n",
    "        diff_cols_a = [f\"B_{i}\" for i in [11, 14, 17]] + [\"D_39\", \"D_131\"] + [f\"S_{i}\" for i in [16, 23]]\n",
    "        diff_cols_b = [\"P_2\", \"P_3\"]\n",
    "        diff_feat = dataset[diff_cols_a + diff_cols_b]\n",
    "        for a in diff_cols_a:\n",
    "            for b in diff_cols_b:\n",
    "                    diff_feat[f\"{a}-{b}\"] = diff_feat[a] - diff_feat[b]\n",
    "        diff_feat.drop(diff_cols_a + diff_cols_b, axis=1, inplace=True)\n",
    "        diff_feat = diff_feat.groupby(level='customer_ID').agg(['first', 'last', 'mean', 'std']).pipe(flatten_columns)\n",
    "\n",
    "        dataset = cudf.concat([cat_feat, num_feat, diff_feat], axis=1)\n",
    "        return dataset\n",
    "\n",
    "\n",
    "def predict(customers, rows, num_cust):\n",
    "    skip_rows = 0\n",
    "    skip_cust = 0\n",
    "    test_preds = []\n",
    "\n",
    "    for k in range(len(rows)):\n",
    "        print(\"#\" * 25)\n",
    "        print(f\"### {k}\")\n",
    "        print(\"#\" * 25)\n",
    "        \n",
    "        test = cudf.read_parquet(\n",
    "            \"../input/amex-data-integer-dtypes-parquet-format/test.parquet\",\n",
    "            skiprows=skip_rows, num_rows=rows[k]\n",
    "        )\n",
    "\n",
    "        test['customer_ID'] = test['customer_ID'].str[-16:].str.hex_to_int().astype('int64')\n",
    "        test['S_2'] = cudf.to_datetime(test['S_2'])\n",
    "        test.set_index(['customer_ID', 'S_2'], inplace=True)\n",
    "\n",
    "        gc.collect()\n",
    "        skip_rows += rows[k]\n",
    "\n",
    "        test = engineer(test, 7)\n",
    "        \n",
    "        if k == len(rows) - 1:\n",
    "            test = test.loc[customers[skip_cust:]]\n",
    "        else:\n",
    "            test = test.loc[customers[skip_cust : skip_cust + num_cust]]\n",
    "        \n",
    "        skip_cust += num_cust\n",
    "\n",
    "        # Prepare data for inference\n",
    "        dtest = xgb.DMatrix(data=test)\n",
    "        gc.collect()\n",
    "\n",
    "        # Compute predictions and average blend all fold models\n",
    "        model = xgb.Booster()\n",
    "        model.load_model(f\"xgb_fold0_seed42.xgb\")\n",
    "        preds = model.predict(dtest)\n",
    "        for f in range(1, 5):\n",
    "            model.load_model(f\"xgb_fold{f}_seed42.xgb\")\n",
    "            preds += model.predict(dtest, iteration_range=(0, model.best_iteration + 1))\n",
    "        preds /= 5\n",
    "        test_preds.append(preds)\n",
    "\n",
    "        # Cleanup\n",
    "        del dtest, model\n",
    "        _ = gc.collect()\n",
    "\n",
    "    return test_preds\n",
    "\n",
    "\n",
    "def main():\n",
    "    test_customers = cudf.read_parquet(\n",
    "        \"../input/amex-data-integer-dtypes-parquet-format/test.parquet\",\n",
    "        columns=['customer_ID']\n",
    "    )\n",
    "    test_customers[\"customer_ID\"] = test_customers[\"customer_ID\"].str[-16:].str.hex_to_int().astype(\"int64\")\n",
    "    \n",
    "    def get_rows(customers, test, num_parts):\n",
    "        \"\"\"Divides the test dataset in `num_parts` parts.\n",
    "        Each part contains approximately `chunk` customers.\n",
    "        Returns the number of rows and then number of customers in\n",
    "        each part, except the last which has fewer.\n",
    "        \"\"\"\n",
    "        chunk = len(customers) // num_parts\n",
    "        rows = []\n",
    "\n",
    "        for k in range(num_parts):\n",
    "            if k == num_parts - 1:\n",
    "                cc = customers[k * chunk :]\n",
    "            else:\n",
    "                cc = customers[k * chunk : (k + 1) * chunk]\n",
    "\n",
    "            s = test.loc[test.customer_ID.isin(cc)].shape[0]\n",
    "            rows.append(s)\n",
    "\n",
    "        return rows, chunk\n",
    "    \n",
    "    customers = test_customers[[\"customer_ID\"]].drop_duplicates().sort_index().values.flatten()\n",
    "    rows, num_cust = get_rows(customers, test_customers[[\"customer_ID\"]], num_parts=10)\n",
    "    \n",
    "    test_preds = predict(customers, rows, num_cust)\n",
    "    \n",
    "    test_preds = np.concatenate(test_preds)\n",
    "    test = cudf.DataFrame(index=customers, data={\"prediction\": test_preds})\n",
    "    sub = cudf.read_csv(\"../input/amex-default-prediction/sample_submission.csv\")[\n",
    "        [\"customer_ID\"]\n",
    "    ]\n",
    "    sub[\"customer_ID_hash\"] = sub[\"customer_ID\"].str[-16:].str.hex_to_int().astype(\"int64\")\n",
    "    sub = sub.set_index(\"customer_ID_hash\")\n",
    "    sub = sub.merge(test[[\"prediction\"]], left_index=True, right_index=True, how=\"left\")\n",
    "    sub = sub.reset_index(drop=True)\n",
    "\n",
    "    # Display predictions\n",
    "    sub.to_csv(f\"submission_xgb.csv\", index=False)\n",
    "    print(\"Submission file shape is\", sub.shape)\n",
    "\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "502a4a7c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-26T20:39:19.063038Z",
     "iopub.status.busy": "2022-07-26T20:39:19.062691Z",
     "iopub.status.idle": "2022-07-26T20:39:19.747606Z",
     "shell.execute_reply": "2022-07-26T20:39:19.746442Z"
    },
    "papermill": {
     "duration": 0.701349,
     "end_time": "2022-07-26T20:39:19.750132",
     "exception": false,
     "start_time": "2022-07-26T20:39:19.048783",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 142940\r\n",
      "drwxr-xr-x 2 root root     4096 Jul 26 20:39 .\r\n",
      "drwxr-xr-x 6 root root     4096 Jul 26 20:03 ..\r\n",
      "---------- 1 root root    73921 Jul 26 20:39 __notebook__.ipynb\r\n",
      "-rw-r--r-- 1 root root 71094059 Jul 26 20:39 submission_xgb.csv\r\n",
      "-rw-r--r-- 1 root root 15577894 Jul 26 20:10 xgb_fold0_seed42.xgb\r\n",
      "-rw-r--r-- 1 root root 16471022 Jul 26 20:16 xgb_fold1_seed42.xgb\r\n",
      "-rw-r--r-- 1 root root 13484194 Jul 26 20:21 xgb_fold2_seed42.xgb\r\n",
      "-rw-r--r-- 1 root root 14857390 Jul 26 20:26 xgb_fold3_seed42.xgb\r\n",
      "-rw-r--r-- 1 root root 14785394 Jul 26 20:31 xgb_fold4_seed42.xgb\r\n"
     ]
    }
   ],
   "source": [
    "!ls -al"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 2156.336834,
   "end_time": "2022-07-26T20:39:22.094837",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-07-26T20:03:25.758003",
   "version": "2.3.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
