{"cells":[{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2025-03-26T06:56:08.050872Z","iopub.status.busy":"2025-03-26T06:56:08.050582Z","iopub.status.idle":"2025-03-26T06:56:11.691799Z","shell.execute_reply":"2025-03-26T06:56:11.690809Z","shell.execute_reply.started":"2025-03-26T06:56:08.050840Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: polars in /usr/local/lib/python3.10/site-packages (1.26.0)\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n","\u001b[0mRequirement already satisfied: pip in /usr/local/lib/python3.10/site-packages (25.0.1)\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n","\u001b[0m"]}],"source":["#pip installations here\n","!pip install polars\n","!pip install --upgrade pip"]},{"cell_type":"code","execution_count":4,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2025-03-26T06:57:01.848561Z","iopub.status.busy":"2025-03-26T06:57:01.848203Z","iopub.status.idle":"2025-03-26T06:57:06.965842Z","shell.execute_reply":"2025-03-26T06:57:06.965069Z","shell.execute_reply.started":"2025-03-26T06:57:01.848527Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["/kaggle/input/amex-default-prediction/sample_submission.csv\n","/kaggle/input/amex-default-prediction/train_data.csv\n","/kaggle/input/amex-default-prediction/test_data.csv\n","/kaggle/input/amex-default-prediction/train_labels.csv\n"]}],"source":["# This Python 3 environment comes with many helpful analytics libraries installed\n","# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n","# For example, here's several helpful packages to load\n","\n","import numpy as np # linear algebra\n","#import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","\n","import polars as pl\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import pandas as pd\n","\n","# Input data files are available in the read-only \"../input/\" directory\n","# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n","\n","import os\n","for dirname, _, filenames in os.walk('/amex-dataset'):\n","    for filename in filenames:\n","        print(os.path.join(dirname, filename))\n","\n","# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n","# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"]},{"cell_type":"markdown","metadata":{},"source":["# initializing variables"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2025-03-26T06:57:10.729096Z","iopub.status.busy":"2025-03-26T06:57:10.728124Z","iopub.status.idle":"2025-03-26T06:57:10.732750Z","shell.execute_reply":"2025-03-26T06:57:10.732113Z","shell.execute_reply.started":"2025-03-26T06:57:10.729058Z"},"trusted":true},"outputs":[],"source":["# CONSTANTS\n","MISSINGNESS_THRESHOLD = 0.99\n","correlation_threshold=0.9\n","point_biserial_threshold=0.01\n","stat_sig=0.05\n","\n","# List of specified categorical columns\n","categorical_columns = ['B_30', 'B_38', 'D_114', 'D_116', 'D_117', 'D_120', 'D_126', 'D_63', 'D_64', 'D_66', 'D_68','customer_ID','S_2']"]},{"cell_type":"markdown","metadata":{},"source":["# Reading training data and identifying important columns"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2025-03-26T06:57:13.262221Z","iopub.status.busy":"2025-03-26T06:57:13.261921Z","iopub.status.idle":"2025-03-26T07:00:10.723710Z","shell.execute_reply":"2025-03-26T07:00:10.722911Z","shell.execute_reply.started":"2025-03-26T06:57:13.262193Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["['customer_ID', 'S_2', 'P_2', 'D_39', 'B_1', 'B_2', 'R_1', 'S_3', 'D_41', 'B_3', 'D_42', 'D_43', 'D_44', 'B_4', 'D_45', 'B_5', 'R_2', 'D_46', 'D_47', 'D_48', 'D_49', 'B_6', 'B_7', 'B_8', 'D_50', 'D_51', 'B_9', 'R_3', 'D_52', 'P_3', 'B_10', 'D_53', 'S_5', 'B_11', 'S_6', 'D_54', 'R_4', 'S_7', 'B_12', 'S_8', 'D_55', 'D_56', 'B_13', 'R_5', 'D_58', 'S_9', 'B_14', 'D_59', 'D_60', 'D_61', 'B_15', 'S_11', 'D_62', 'D_63', 'D_64', 'D_65', 'B_16', 'B_17', 'B_18', 'B_19', 'D_66', 'B_20', 'D_68', 'S_12', 'R_6', 'S_13', 'B_21', 'D_69', 'B_22', 'D_70', 'D_71', 'D_72', 'S_15', 'B_23', 'D_73', 'P_4', 'D_74', 'D_75', 'D_76', 'B_24', 'R_7', 'D_77', 'B_25', 'B_26', 'D_78', 'D_79', 'R_8', 'R_9', 'S_16', 'D_80', 'R_10', 'R_11', 'B_27', 'D_81', 'D_82', 'S_17', 'R_12', 'B_28', 'R_13', 'D_83', 'R_14', 'R_15', 'D_84', 'R_16', 'B_29', 'B_30', 'S_18', 'D_86', 'D_87', 'R_17', 'R_18', 'D_88', 'B_31', 'S_19', 'R_19', 'B_32', 'S_20', 'R_20', 'R_21', 'B_33', 'D_89', 'R_22', 'R_23', 'D_91', 'D_92', 'D_93', 'D_94', 'R_24', 'R_25', 'D_96', 'S_22', 'S_23', 'S_24', 'S_25', 'S_26', 'D_102', 'D_103', 'D_104', 'D_105', 'D_106', 'D_107', 'B_36', 'B_37', 'R_26', 'R_27', 'B_38', 'D_108', 'D_109', 'D_110', 'D_111', 'B_39', 'D_112', 'B_40', 'S_27', 'D_113', 'D_114', 'D_115', 'D_116', 'D_117', 'D_118', 'D_119', 'D_120', 'D_121', 'D_122', 'D_123', 'D_124', 'D_125', 'D_126', 'D_127', 'D_128', 'D_129', 'B_41', 'B_42', 'D_130', 'D_131', 'D_132', 'D_133', 'R_28', 'D_134', 'D_135', 'D_136', 'D_137', 'D_138', 'D_139', 'D_140', 'D_141', 'D_142', 'D_143', 'D_144', 'D_145']\n"]}],"source":["# Read the CSV \n","df_train = pd.read_csv('train_data.csv')\n","print(df_train.columns)"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2025-03-26T07:01:41.462477Z","iopub.status.busy":"2025-03-26T07:01:41.461831Z","iopub.status.idle":"2025-03-26T07:01:41.469559Z","shell.execute_reply":"2025-03-26T07:01:41.468887Z","shell.execute_reply.started":"2025-03-26T07:01:41.462441Z"},"trusted":true},"outputs":[{"data":{"text/plain":["'pd.set_option(\\'display.max_rows\\', None)\\n\\n# Create DataFrame with column names and data types\\ndf_dtypes = pd.DataFrame({\"Column Name\": df_train.columns, \"Data Type\": df_train.dtypes})\\n\\n# Print full DataFrame\\nprint(df_dtypes)\\n\\n# Reset display option if needed\\npd.reset_option(\\'display.max_rows\\')'"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["#show all data \n","# Ensure all rows are printed\n","\"\"\"pd.set_option('display.max_rows', None)\n","\n","# Create DataFrame with column names and data types\n","df_dtypes = pd.DataFrame({\"Column Name\": df_train.columns, \"Data Type\": df_train.dtypes})\n","\n","# Print full DataFrame\n","print(df_dtypes)\n","\n","# Reset display option if needed\n","pd.reset_option('display.max_rows')\"\"\""]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2025-03-26T07:01:11.242277Z","iopub.status.busy":"2025-03-26T07:01:11.241733Z","iopub.status.idle":"2025-03-26T07:01:12.993598Z","shell.execute_reply":"2025-03-26T07:01:12.992765Z","shell.execute_reply.started":"2025-03-26T07:01:11.242239Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Column: B_30\n","shape: (4,)\n","Series: 'B_30' [f64]\n","[\n","\tnull\n","\t0.0\n","\t1.0\n","\t2.0\n","]\n","--------------------------------------------------\n","Column: B_38\n","shape: (8,)\n","Series: 'B_38' [f64]\n","[\n","\tnull\n","\t1.0\n","\t2.0\n","\t3.0\n","\t4.0\n","\t5.0\n","\t6.0\n","\t7.0\n","]\n","--------------------------------------------------\n","Column: D_114\n","shape: (3,)\n","Series: 'D_114' [f64]\n","[\n","\tnull\n","\t0.0\n","\t1.0\n","]\n","--------------------------------------------------\n","Column: D_116\n","shape: (3,)\n","Series: 'D_116' [f64]\n","[\n","\tnull\n","\t0.0\n","\t1.0\n","]\n","--------------------------------------------------\n","Column: D_117\n","shape: (8,)\n","Series: 'D_117' [f64]\n","[\n","\tnull\n","\t-1.0\n","\t1.0\n","\t2.0\n","\t3.0\n","\t4.0\n","\t5.0\n","\t6.0\n","]\n","--------------------------------------------------\n","Column: D_120\n","shape: (3,)\n","Series: 'D_120' [f64]\n","[\n","\tnull\n","\t0.0\n","\t1.0\n","]\n","--------------------------------------------------\n","Column: D_126\n","shape: (4,)\n","Series: 'D_126' [f64]\n","[\n","\tnull\n","\t-1.0\n","\t0.0\n","\t1.0\n","]\n","--------------------------------------------------\n","Column: D_63\n","shape: (6,)\n","Series: 'D_63' [str]\n","[\n","\t\"XM\"\n","\t\"CL\"\n","\t\"XL\"\n","\t\"CO\"\n","\t\"XZ\"\n","\t\"CR\"\n","]\n","--------------------------------------------------\n","Column: D_64\n","shape: (5,)\n","Series: 'D_64' [str]\n","[\n","\t\"-1\"\n","\tnull\n","\t\"U\"\n","\t\"O\"\n","\t\"R\"\n","]\n","--------------------------------------------------\n","Column: D_66\n","shape: (3,)\n","Series: 'D_66' [f64]\n","[\n","\tnull\n","\t0.0\n","\t1.0\n","]\n","--------------------------------------------------\n","Column: D_68\n","shape: (8,)\n","Series: 'D_68' [f64]\n","[\n","\tnull\n","\t0.0\n","\t1.0\n","\t2.0\n","\t3.0\n","\t4.0\n","\t5.0\n","\t6.0\n","]\n","--------------------------------------------------\n","Column: customer_ID\n","shape: (458_913,)\n","Series: 'customer_ID' [str]\n","[\n","\t\"a4c2df0a9237a097f1a3a24aa0f2f7…\n","\t\"80391ab4594dc38a2e50350e86f189…\n","\t\"fe2c553c41133dd1d18e1f843d787a…\n","\t\"55886ed3662f97ad376d074ccbb5e0…\n","\t\"d5bbf579d5f1f984df6dd3136b7a61…\n","\t…\n","\t\"38b3c12e8dac99d65fcdda381a5e67…\n","\t\"3c3408dcd2eb0b04ce3eb7ad6f76f7…\n","\t\"b379b2591412d834c20807053d092d…\n","\t\"fc954f6f437e9517c3401a1e48508b…\n","\t\"22ed52195c96d0bdd69b8ceb669701…\n","]\n","--------------------------------------------------\n","Column: S_2\n","shape: (396,)\n","Series: 'S_2' [str]\n","[\n","\t\"2017-11-06\"\n","\t\"2018-01-06\"\n","\t\"2017-10-15\"\n","\t\"2017-11-14\"\n","\t\"2017-06-08\"\n","\t…\n","\t\"2017-10-17\"\n","\t\"2017-12-05\"\n","\t\"2017-08-30\"\n","\t\"2017-05-21\"\n","\t\"2017-12-01\"\n","]\n","--------------------------------------------------\n"]}],"source":["#identify the unique values of the categorical columns\n","# Loop through each column and print its unique values\n","for col in categorical_columns:\n","    if col in df_train.columns:  # Ensure the column exists in df_train\n","        print(f\"Column: {col}\")\n","        print(df_train[col].unique())  # Print unique values\n","        print(\"-\" * 50)\n","    else:\n","        print(f\"Warning: Column '{col}' not found in df_train\")"]},{"cell_type":"markdown","metadata":{},"source":["NOTE:\n","\n","train_data.csv Data types are mainly in float64 and string \n","\n","customerID: string \n","\n","S-2(date): string\n","( we can see that the train statement dates ranges from \"2017-12-23\" to \"2018-03-14\" this show )\n","\n","categorical data colums:\n","['B_30', 'B_38', 'D_114', 'D_116', 'D_117', 'D_120', 'D_126', 'D_63', 'D_64', 'D_66', 'D_68']\n","note: D_63,D_64 : string\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2025-03-26T07:11:16.485545Z","iopub.status.busy":"2025-03-26T07:11:16.484306Z"},"trusted":true},"outputs":[],"source":["# Convert 'S_2' to datetime format\n","df_train[\"S_2\"] = pd.to_datetime(df_train[\"S_2\"], format=\"%Y-%m-%d\")\n","\n","# 1. Count UNIQUE statements per customer\n","unique_counts = (\n","    df_train.drop_duplicates(subset=[\"customer_ID\", \"S_2\"])  # Remove duplicate dates per customer\n","    .groupby(\"customer_ID\")\n","    .size()\n","    .reset_index(name=\"statement_count\")\n","    .sort_values(\"statement_count\", ascending=False)\n",")\n","\n","# 2. Create pie chart\n","plt.figure(figsize=(10, 8))\n","colors = plt.cm.tab20.colors\n","\n","plt.pie(\n","    unique_counts[\"statement_count\"],\n","    labels=unique_counts[\"customer_ID\"],\n","    autopct=lambda p: f\"{p:.1f}%\\n({int(p*sum(unique_counts['statement_count'])/100)})\",\n","    startangle=90,\n","    colors=colors,\n","    wedgeprops={\"edgecolor\": \"white\", \"linewidth\": 1}\n",")\n","\n","plt.title(\"Unique Bank Statements Distribution by Customer\\n(Polars + Matplotlib)\", pad=20)\n","plt.axis(\"equal\")\n","\n","# 3. Add table with exact counts\n","plt.table(\n","    cellText=unique_counts.values,\n","    colLabels=unique_counts.columns,\n","    loc=\"bottom\",\n","    bbox=[0, -0.4, 1, 0.3]\n",")\n","\n","plt.tight_layout()\n","plt.show()\n","\n","# Print the raw data\n","print(\"Unique statement counts per customer:\")\n","print(unique_counts)"]},{"cell_type":"markdown","metadata":{},"source":["# Missing values removal functions"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["#gets the colums with 99% missing data\n","def missing_columns(data):\n","    missing_percentages = data.isnull().mean()\n","    missing_columns = missing_percentages[missing_percentages > MISSINGNESS_THRESHOLD].index\n","    return missing_columns\n","    \n","#removes colums with 99% of data missing ^takes colums from above function\n","def drop_columns_with_many_missing_values_in_derived_dataset(data,base_columns_to_drop):\n","  columns_to_drop=[]\n","  for i in range(0,len(base_columns_to_drop)):\n","    columns_to_drop.append(base_columns_to_drop[i]+'_')\n","  columns_to_drop = [col for col in data.columns if (any(word in col for word in columns_to_drop) and (\"last\" not in col))]\n","  data_filtered = data.drop(columns=columns_to_drop, errors='ignore')\n","  return data_filtered"]},{"cell_type":"markdown","metadata":{},"source":["# Handling missing colums"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["#show the bad cols\n","list_of_bad_col = missing_columns(df_train)\n","list_of_bad_col"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# removing the bad colums\n","df_train = drop_columns_with_many_missing_values_in_derived_dataset(df_train,list_of_bad_col)\n","df_train.shape"]},{"cell_type":"markdown","metadata":{},"source":["# aggregating new data"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def enhanced_feature_engineering(df_train):\n","    \"\"\"\n","    Performs comprehensive feature engineering on financial data.\n","    \n","    Args:\n","        df: Input DataFrame containing financial features\n","        \n","    Returns:\n","        DataFrame with original and engineered features\n","    \"\"\"\n","    # Data Cleaning\n","    df_train = df_train.replace([np.inf, -np.inf], np.nan)\n","    df_train = df_train.fillna(-127)  # Consider using a more meaningful null placeholder\n","    \n","    # Feature Identification\n","    non_feature_cols = ['customer_ID', 'S_2', 'target']\n","    num_features = [col for col in df_train.columns if col not in non_feature_cols]\n","    \n","    \n","    # Numerical Transformations (now applies to all numerical features)\n","    num_features = [col for col in num_features if col not in categorical_columns]\n","    for col in num_features:\n","        if df_train[col].dtype in ['float64', 'int64']:\n","            # Skip log transform if negative values exist\n","            if (df_train[col] >= 0).all():\n","                df_train[f'{col}_log'] = np.log1p(df_train[col])\n","    \n","    # Feature Groupings (now includes all features, not just *_last)\n","    P_columns = [col for col in df_train.columns if col.startswith('P_')]\n","    S_columns = [col for col in df_train.columns if col.startswith('S_')]\n","    B_columns = [col for col in df_train.columns if col.startswith('B_')]\n","    D_columns = [col for col in df_train.columns if col.startswith('D_')]\n","    R_columns = [col for col in df_train.columns if col.startswith('R_')]\n","    \n","    # 6. Aggregate Features with safer calculations\n","    df_train['total_payments'] = df_train[P_columns].sum(axis=1)\n","    df_train['total_spending'] = df_train[S_columns].sum(axis=1)\n","    df_train['total_balance'] = df_train[B_columns].sum(axis=1)\n","    df_train['total_delinquency'] = df_train[D_columns].sum(axis=1)\n","    df_train['total_risk'] = df_train[R_columns].sum(axis=1)\n","    \n","    # 7. Ratio Features with zero-division protection\n","    epsilon = 1e-6\n","    df_train['risk_to_payment_ratio'] = np.where(\n","        df_train['total_payments'].abs() > epsilon,\n","        df_train['total_risk'] / (df_train['total_payments'] + epsilon),\n","        np.nan\n","    )\n","    df_train['delinquency_to_balance_ratio'] = np.where(\n","        df_train['total_balance'].abs() > epsilon,\n","        df_train['total_delinquency'] / (df_train['total_balance'] + epsilon),\n","        np.nan\n","    )\n","    \n","    # 8. Memory Optimization\n","    for col in df_train.select_dtypes(['float64']).columns:\n","        if col != \"customer_ID\":\n","            df_train[col] = df_train[col].astype(np.float32)\n","            \n","    for col in df_train.select_dtypes(['int64']).columns:\n","        if col != \"customer_ID\":\n","            df_train[col] = df_train[col].astype(np.int32)\n","    \n","    print(f'Shape after feature engineering: {df_train.shape}')\n","    print(f'Number of new features created: {len(df_train.columns) - len(non_feature_cols)}')\n","    \n","    return df_train"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["df_train = enhanced_feature_engineering(df_train)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["missing_columns_derived=missing_columns(df_train)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["print(missing_columns_derived)"]},{"cell_type":"markdown","metadata":{},"source":["# Reducing number of features using correlation"]},{"cell_type":"markdown","metadata":{},"source":["# *handling low correlation data*"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["\n","from scipy.stats import pointbiserialr\n","\n","def find_low_point_biserial_correlations_columns(df, target_column, category, suffix):\n","\n","    df.replace([np.inf, -np.inf], np.nan, inplace=True)\n","    df.fillna(-127, inplace=True)\n","\n","\n","    columns_to_see = [col for col in df.columns if (col.startswith(category) and col.endswith(suffix)) or col == target_column]\n","    df = df[columns_to_see]\n","\n","    correlation_results = []\n","\n","\n","    for col in columns_to_see:\n","        if col != target_column:\n","            correlation, p_value = pointbiserialr(df[target_column], df[col])\n","            correlation_results.append((col, correlation, p_value))\n","\n","\n","    correlation_df = pd.DataFrame(correlation_results, columns=['Feature', 'Point-Biserial Correlation', 'p-value'])\n","\n","\n","    filtered_df = correlation_df[(correlation_df['Point-Biserial Correlation'].abs() < point_biserial_threshold) & (correlation_df['p-value'] < stat_sig)]\n","\n","\n","    return filtered_df['Feature'].tolist()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["\n","category= [\"B\",\"P\",\"S\",\"D\",\"R\"]\n","suffix=['first', 'mean', 'std', 'min', 'max']"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["\n","def remove_low_correlation_columns(df):\n","    all_cols_to_omit=[]\n","    for c in category:\n","        for s in suffix:\n","            columns_to_omit=find_low_point_biserial_correlations_columns(df,\"target\",c,s)\n","            all_cols_to_omit.append(columns_to_omit)\n","    return all_cols_to_omit"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["\n","all_low_correlation_columns=remove_low_correlation_columns(df_train)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["\n","for l in all_low_correlation_columns:\n","  df_train.drop(columns=l, errors='ignore',inplace=True)"]},{"cell_type":"markdown","metadata":{},"source":["# *handling high correlated data*"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from scipy.stats import pearsonr\n","\n","def find_highly_correlated_columns(df, category, suffix):\n","    # df=df.to_pandas()\n","    df.replace([np.inf, -np.inf], np.nan, inplace=True)\n","    df.fillna(-127, inplace=True)\n","\n","\n","    numerical_columns = [col for col in df.columns if col.startswith(category) and col.endswith(suffix)]\n","    df = df[numerical_columns]\n","\n","\n","    correlation_matrix = df.corr(method='pearson').abs()\n","\n","\n","    correlated_pairs = set()\n","    columns_to_remove = set()\n","\n","    for i in range(len(correlation_matrix.columns)):\n","        for j in range(i + 1, len(correlation_matrix.columns)):\n","            if correlation_matrix.iloc[i, j] > correlation_threshold:\n","                col1 = correlation_matrix.columns[i]\n","                col2 = correlation_matrix.columns[j]\n","\n","\n","                correlated_pairs.add((col1, col2))\n","\n","                columns_to_remove.add(col2)\n","\n","    return list(columns_to_remove)\n","\n","def remove_high_correlation_columns(df):\n","    all_cols_to_omit=[]\n","    for c in category:\n","        for s in suffix:\n","            columns_to_omit=find_highly_correlated_columns(df,c,s)\n","            all_cols_to_omit.append(columns_to_omit)\n","    return all_cols_to_omit"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["\n","all_high_correlation_columns=remove_high_correlation_columns(df_train)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["\n","for l in all_high_correlation_columns:\n","  df_train.drop(columns=l, errors='ignore',inplace=True)"]},{"cell_type":"markdown","metadata":{},"source":["test"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["cat_features = ['B_30', 'B_38', 'D_114', 'D_116', 'D_117', 'D_120', 'D_126', 'D_63', 'D_64', 'D_66', 'D_68','customer_ID','S_2','target']"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["\n","df_sampled = df_train.sample(n=20000, seed=42)\n","\n","df_numeric = df_sampled.drop(cat_features)\n","\n","correlation_matrix = df_numeric.corr()\n","\n","print(correlation_matrix)\n","\n","print(type(correlation_matrix))"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":["# Removing Outliers"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["\n","train_data = df_train"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["train_data_cat = train_data[cat_features]"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["train_data_num = train_data.drop(cat_features)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["for col in train_data_num.columns:\n","    if col not in cat_features:\n","        \n","        Q1 = train_data_num.select(pl.col(col).quantile(0.25)).to_numpy()[0][0]\n","        Q3 = train_data_num.select(pl.col(col).quantile(0.75)).to_numpy()[0][0]\n","        IQR = Q3 - Q1\n","        \n","        # Calculate lower and upper bounds for outliers\n","        lower_bound = Q1 - 1.5 * IQR\n","        upper_bound = Q3 + 1.5 * IQR\n","\n","        # Filter the DataFrame to keep only rows within bounds\n","        train_data_num = train_data_num.filter(pl.col(col).is_between(lower_bound, upper_bound))\n","\n","# After the loop, train_data will contain only rows without outliers in any specified column\n","print(train_data_num)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# 4. Standardize the numerical columns (mean=0, std=1)\n","train_data_num = (train_data_num - train_data_num.mean()) / train_data_num.std()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["train_data_num"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# 5. Concatenate the numerical and categorical data\n","train_data = pd.concat([train_data_cat, train_data_num], axis=1)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["train_data.head()"]},{"cell_type":"markdown","metadata":{},"source":["# Handling Categorical Variables "]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# train_data_cat.null_count()\n","# print(500/len(train_data))"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["train_data = train_data.dropna()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["train_data['target'].value_counts()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["train_data_cp = train_data"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["df_majority = df_train[df_train['target'] == 0]\n","df_minority = df_train[df_train['target'] == 1]\n","\n","# Determine the number of samples for upsampling\n","n_samples = len(df_majority)\n","\n","# Upsample minority class\n","df_minority_upsampled = df_minority.sample(\n","    n=n_samples, \n","    replace=True,  # Replace to allow upsampling\n","    random_state=42  # Set the seed for reproducibility\n",")\n","# Combine majority and upsampled minority classes\n","df_upsampled = pd.concat([df_majority, df_minority_upsampled])\n","\n","print(df_upsampled)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["train_data = df_upsampled"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["train_data['target'].value_counts()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["train_data_cat_encoded = pd.get_dummies(train_data_cat)\n","print(train_data_cat_encoded)"]}],"metadata":{"kaggle":{"accelerator":"tpu1vmV38","dataSources":[{"databundleVersionId":3723648,"sourceId":35332,"sourceType":"competition"}],"dockerImageVersionId":30806,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.4"}},"nbformat":4,"nbformat_minor":4}
